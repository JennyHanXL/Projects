#Using machine learning to predict Active vs Inactive customers
#https://trachack.com/challenge-20-1/



import argparse

from pyspark.sql import SparkSession
from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler
from pyspark.ml import Pipeline
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import BinaryClassificationEvaluator
import pyspark.sql.functions as func
from pyspark.sql.types import IntegerType
from pyspark.sql.functions import sum as _sum
from pyspark.sql.functions import min as _min
from pyspark.sql.functions import max as _max
from datetime import datetime
from pyspark.ml.classification import RandomForestClassifier, GBTClassifier



def get_arg_paser():
    """Returns the argument parer object."""
    # Create the parser
    parser = argparse.ArgumentParser(prog='trachack-submssion',
                                     description='A TracHack solution submission template. '
                                                 'The script provides a way for participants to submit their solution '
                                                 'for the automated testing framework. '
                                                 'The pyspark script takes input to dataset and outputs the ROC AUC '
                                                 'values for each of the folds and the averaged AUC across the folds.'
                                                 'Modify select_data(), featurize_data() and train_model() with the '
                                                 'code for your solution.')

    # Add the arguments
    parser.add_argument('-d',
                        '--data',
                        metavar='data',
                        type=str,
                        required=True,
                        help='The path to dataset to train and evaluate the model.')

    parser.add_argument('-s',
                        '--seed',
                        metavar='seed',
                        type=int,
                        default=123,
                        help='The random split seed generator for reproducibility during train/test data split.')
    return parser


def select_data(data_frame):
    """Selects and transforms the raw GCR (JSON) records data frame into a data frame.

    :param data_frame: Input data frame to select the specific data elements from.
    :returns selected dataframe that includes the columns of interest.
    """
    # filter customers info between '2018-03-18' and '2020-03-19'
    df = data_frame.filter((data_frame['customer_info.first_activation_date'] < '2020-03-19')&(data_frame['customer_info.first_activation_date'] >= '2018-03-18'))
    #customer_info
    df_customer_info = df.select(func.col("customer_info.line_id").alias("line_id"),func.col("customer_info.first_activation_date").alias("first_activation_date"),func.col("customer_info.lifetime_redemptions").alias("lifetime_redemptions"),func.col("customer_info.lifetime_revenues").alias("lifetime_revenues"))
    
    #active&inactive tenure
    df_status = df.select(func.col("customer_info.line_id").alias("line_id"),func.col("status"))
    df_status_active = df_status.filter(df_status['status']=='Active')
    active_first_date = df.select(func.col("customer_info.line_id").alias("line_id"),func.col("customer_info.first_activation_date").alias("first_date"))
    df_active_join = df_status_active.join(active_first_date,df_status_active.line_id == active_first_date.line_id,how='left').drop(active_first_date.line_id)
    df_active_join = df_active_join.withColumn("tenure_active",func.datediff(func.to_date(func.lit("2020-03- 19")),func.to_date("first_date")))
    df_active_join_id = df_active_join.groupby("line_id").agg(_max("tenure_active").alias("tenure"))
    df_status_inactive = df_status.filter(df_status['status']=='Inactive')
    inactive_date = df.select(func.col("customer_info.line_id").alias("line_id"),func.col("customer_info.first_activation_date").alias("first_date"),func.explode("deactivations.deactivation_date").alias("deactivation_date"))
    df_inactive_join = df_status_inactive.join(inactive_date,df_status_inactive.line_id==inactive_date.line_id,how='left').drop(inactive_date.line_id)
    df_inactive_join =  df_inactive_join.withColumn("tenure_inactive",func.datediff(func.to_date(func.to_date("deactivation_date")),func.to_date("first_date")))
    df_inactive_join_id = df_inactive_join.groupby("line_id").agg(_max("tenure_inactive").alias("tenure"))
    df_concat = df_active_join_id.union(df_inactive_join_id)
    
    """network_usage_domestic"""
    df_net_usage = df.select(func.col("customer_info.line_id").alias("line_id"),func.explode("network-usage-domestic").alias("net_usage_by_day"))
    df_net_usage_table = df_net_usage.select("line_id", "net_usage_by_day.*")
    df_net_usage_table = df_net_usage_table.withColumn("sms", func.col("sms_in") + func.col("sms_out"))
    df_net_usage_table = df_net_usage_table.withColumn("voice_duration", func.col("voice_duration_in") + func.col("voice_duration_out"))
    df_net_usage_table=df_net_usage_table.groupBy("line_id").agg(_sum("sms").alias("sms_id"),_sum("voice_duration").alias("voice_duration_id"),_sum("voice_num_total").alias("voice_num_total_id"))
    net_join_concat = df_concat.join(df_net_usage_table,df_net_usage_table.line_id == df_concat.line_id,how='left').drop(df_net_usage_table.line_id)
    df_net_usage_features = net_join_concat.withColumn("sms_ave_id", func.col("sms_id") / func.col("tenure"))
    df_net_usage_features = df_net_usage_features.withColumn("voice_duration_ave_id", func.col("voice_duration_id") / func.col("tenure"))
    df_net_usage_features = df_net_usage_features.withColumn("voice_num_ave_id", func.col("voice_num_total_id") / func.col("tenure"))
    
    """phone_info"""
    df_phone_info_old = df.select(func.col("customer_info.line_id").alias("line_id"),func.col("phone_info.phone_gen").alias("phone_gen"),func.col("phone_info.operating_system").alias("operating_system"))
    df_phone_info = df_phone_info_old.withColumn("update_col",func.when((func.col("operating_system") == "BYOP") | (func.col("operating_system") == "BYOD"), "BYOP(D)").when((func.col("operating_system") == "IOS") | (func.col("operating_system") == "PROPRIETARY"), "IOS_PTY").otherwise(func.col("operating_system")))
    
        
    """support_history"""
    df_support_history= df.select(func.col("customer_info.line_id").alias("line_id"),func.explode("support_history").alias("support_history"))   
    df_support_history=df_support_history.select("line_id","support_history.*")
    df_support_case_num= df_support_history.groupby("line_id").count()
    df_support_case_num= df_support_case_num.select(func.col("line_id").alias("line_id"),func.col("count").alias("case_num"))
    

    """join"""
    
    customer_net = df_customer_info.join(df_net_usage_features,df_customer_info.line_id==df_net_usage_features.line_id,how='left').drop(df_net_usage_features.line_id)
    customer_net_support = customer_net.join(df_support_case_num,df_support_case_num.line_id ==customer_net.line_id,how='left').drop(df_support_case_num.line_id)
    customer_net_support_phone = df_phone_info.join(customer_net_support,df_phone_info.line_id ==customer_net_support.line_id,how='left').drop(customer_net_support.line_id)

    customer_net_support_phone_status = customer_net_support_phone.join(df_status,customer_net_support_phone.line_id==df_status.line_id,how='left').drop(df_status.line_id)
    
    
    #selection
    
    final_df = customer_net_support_phone_status.select(func.col("update_col").alias("operating_system_new"),func.col("phone_gen"),func.col("case_num"),func.col("lifetime_redemptions"),func.col("lifetime_revenues"),func.col("sms_ave_id"),func.col("voice_duration_ave_id"),func.col("voice_num_ave_id"),func.col("status"))
    # df_selection = final_df.filter(final_df['tenure'].isNotNull())
    # Annotate missing values with 'unknown' for the fields that we are interested in.
    df_selection = final_df.fillna({'operating_system_new': "unknown", 'phone_gen': "unknown",'case_num':0,"lifetime_redemptions":0,"lifetime_revenues":0,'sms_ave_id':0,'voice_duration_ave_id':0,'voice_num_ave_id':0})
    return df_selection
    

def featurize_data(df, remove_orig_cols=True):
    """Given a selected data frame, generate a featurized dataframe.

    Example: Shows how categorical features are handled by first a string indexer and then one-hot encoding.

    if remove_orig_cols is set, only returns a dataframe with two columns - features and label.


    :param df: Input data frame to featurize.
    :param remove_orig_cols: (Default=True) If set we remove the original columns from the returned dataframe.
    :returns featurized dataframe that includes two columns - label and features. If remove_orig_cols=False, includes
    the columns from original input data frame as well.
    """

    stages = []

    categorical_columns = ['operating_system_new', 'phone_gen']
    for column in categorical_columns:
        string_indexer = StringIndexer(inputCol=column, outputCol=column + '_index')
        encoder = OneHotEncoderEstimator(inputCols=[string_indexer.getOutputCol()], outputCols=[column + "_vec"])
        stages += [string_indexer, encoder]

    label_indexer = StringIndexer(inputCol="status", outputCol="label")
    stages += [label_indexer]

    numeric_columns = ["case_num", "lifetime_redemptions","lifetime_revenues","sms_ave_id","voice_duration_ave_id","voice_num_ave_id"]

    assembler_inputs = [c + "_vec" for c in categorical_columns] + numeric_columns
    assembler = VectorAssembler(inputCols=assembler_inputs, outputCol="features")
    stages += [assembler]

    pipeline = Pipeline(stages=stages)
    pipeline_model = pipeline.fit(df)
    df_featurized = pipeline_model.transform(df)
    if remove_orig_cols:
        selected_columns = ['features', 'label']
        return df_featurized.select(selected_columns)
    else:
        return df_featurized


def train_model(train_dataset):
    """Given a featurized training dataset, trains a simple logistic regression model and
    returns the trained model object.

    :param train_dataset: A dataframe with two columns - label and features.
    :returns trained model object.
    """
    lr_recipe = GBTClassifier(labelCol='label', featuresCol='features')
    lr_model = lr_recipe.fit(train_dataset)
    return lr_model


def evaluate_model(model, test_dataset):
    """Given a model and featurized test dataset, returns the auc value.

    :param model: the pyspark.ml model object to evaluate.
    :param test_dataset: A dataframe with two columns - label and features, to evaluate the model.
    :returns AUC under ROC value.

    """
    predictions = model.transform(test_dataset)
    evaluator = BinaryClassificationEvaluator()
    auc = evaluator.evaluate(predictions, {evaluator.metricName: "areaUnderROC"})
    return auc


def main(spark, data_path, random_seed, test_ratio=0.2, num_folds=10):
    """The end to end model pipeline, printing out the AUC value for each fold and the averaged AUC value.

    :param spark: SparkSession object.
    :param data_path: path to the dataset to use for the pipeline.
    :param random_seed: seed as input to the seed selection for randomSplit for train/test split.
    :param test_ratio: (Default=0.2) Percentage of data to use for test/evaluation. Must be between 0 and 1.
    :param num_folds: (Default=10) Number of folds for averaged AUC value.
    :returns: averaged AUC value
    """
    df = spark.read.json(data_path)
    df_selected = select_data(df)
    df_featurized = featurize_data(df_selected)

    # Cache this data frame since we will be doing multiple passes to split, train and evaluate
    df_featurized.cache()

    fold_auc = []
    for i in range(num_folds):
        fold_seed = random_seed * i
        train, test = df_featurized.randomSplit([1.0 - test_ratio, test_ratio], seed=fold_seed)
        model = train_model(train)
        auc = evaluate_model(model, test)
        print(f"Fold {i} AUC: {auc}")
        fold_auc.append(auc)

    average_auc = sum(fold_auc) / num_folds
    print(f"Average AUC: {average_auc}")
    return average_auc


if __name__ == '__main__':
    arg_parser = get_arg_paser()
    args = arg_parser.parse_args()

    data_path = args.data
    seed = args.seed
    print(vars(args))

    spark = SparkSession.builder.appName('trachack-code-submission').enableHiveSupport().getOrCreate()

    auc = main(spark, data_path, seed)
